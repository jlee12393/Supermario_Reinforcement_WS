{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "\"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                4128792   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 150       \n",
      "=================================================================\n",
      "Total params: 4,129,542\n",
      "Trainable params: 4,129,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 24)                4128792   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 150       \n",
      "=================================================================\n",
      "Total params: 4,129,542\n",
      "Trainable params: 4,129,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.render=False\n",
    "        self.load_model=False\n",
    "        \n",
    "        self.state_size= state_size\n",
    "        self.action_size= action_size\n",
    "        \n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate= 0.001\n",
    "        self.epsilon= 1.0\n",
    "        \n",
    "        self.epsilon_decay=0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        self.memory=deque(maxlen=1000)\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        \n",
    "        self.update_target_model()\n",
    "        \n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/supermario_wonseok.h5\")\n",
    "            \n",
    "            \n",
    "    def build_model(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu', kernel_initializer='he_unif_initializorm'))\n",
    "        model.add(Dense(24, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer = Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def update_target_model(self):\n",
    "        \n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        else:\n",
    "            q_value=self.model.predict(state)   # https://keras.io/models/model/\n",
    "            return np.argmax(q_value[0])\n",
    "        \n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            dones.append(mini_batch[i][4])\n",
    "\n",
    "        # 현재 상태에 대한 모델의 큐함수\n",
    "        # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "        target = self.model.predict(states)\n",
    "        target_val = self.target_model.predict(next_states)\n",
    "\n",
    "        # 벨만 최적 방정식을 이용한 업데이트 타깃\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        self.model.fit(states, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    \n",
    "    env = gym.make('ppaquette/SuperMarioBros-1-1-v0')\n",
    "    state_size=env.observation_space.shape[0]*env.observation_space.shape[1]*3 \n",
    "    action_size = 6\n",
    "    \n",
    "    agent= DQNAgent(state_size, action_size)\n",
    "    \n",
    "    scores, episodes = [],[]\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score=0\n",
    "        \n",
    "        state=env.reset()\n",
    "        state=np.reshape(state, [1,state_size])\n",
    "        \n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "                \n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            next_state, reward, done, infor = env.step(action)\n",
    "            next_state=np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            reward= reward if not done or score ==499 else -10\n",
    "            \n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            \n",
    "            if len(agent.memory) >= agent.train_start:\n",
    "                agent.train_model()\n",
    "                \n",
    "                \n",
    "            score+=reward\n",
    "            state= next_state\n",
    "            \n",
    "            if done:\n",
    "                \n",
    "                agent.update_target_model()\n",
    "                \n",
    "                score = score if score ==500 else score +100\n",
    "                \n",
    "                scores.append(score)\n",
    "                \n",
    "                episodes.append(e)\n",
    "                \n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                \n",
    "                pylab.savefig(\"./save_graph/cartpole_dqn.png\")\n",
    "                print(\"episode:\", e, \" score\", score,\" memory length\". len(agent.memory), \"epsilon:\", agent.epsilon)\n",
    "                \n",
    "                \n",
    "                if np.mean(scores[-min(10, len(scores)):])>1000:\n",
    "                    agent.model.save_weights(\"./save_model/supermario_wonseok_dqn_keras.h5\")\n",
    "                    sys.exit()\n",
    "        \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from gym import wrappers\n",
    "\n",
    "input_size = env.observation_space.shape[0]*env.observation_space.shape[1]*3        #####change input_size - 224*256*3 acquired from ppaquette_gym_super_mario/nes_env.py\n",
    "output_size = 6                                                                     #####meaning of output can be found at ppaquette_gym_super_mario/wrappers/action_space.py\n",
    "\n",
    "dis = 0.9\n",
    "REPLAY_MEMORY = 50000\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, session, input_size, output_size, name=\"main\"):\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self, h_size=10, l_rate=1e-1):\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "\n",
    "            # First layer of weights\n",
    "            W1 = tf.get_variable(\"W1\", shape=[self.input_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1 = tf.nn.tanh(tf.matmul(self._X, W1))\n",
    "\n",
    "            # Second layer of Weights\n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, self.output_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            # Q prediction\n",
    "            self._Qpred = tf.matmul(layer1, W2)\n",
    "\n",
    "        # We need to define the parts of the network needed for learning a policy\n",
    "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "\n",
    "        # Loss function\n",
    "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "        # Learning\n",
    "        self._train = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(self._loss)\n",
    "\n",
    "    def predict(self, state):\n",
    "        x = np.reshape(state, [1, self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
    "\n",
    "    def update(self, x_stack, y_stack):\n",
    "        return self.session.run([self._loss, self._train], feed_dict={self._X: x_stack, self._Y: y_stack})\n",
    "\n",
    "def replay_train(mainDQN, targetDQN, train_batch):\n",
    "    x_stack = np.empty(0).reshape(0, input_size)\n",
    "    y_stack = np.empty(0).reshape(0, output_size)\n",
    "\n",
    "    # Get stored information from the buffer\n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q = mainDQN.predic(state)\n",
    "\n",
    "        # terminal?\n",
    "        if done:\n",
    "            Q[0, action] = reward\n",
    "        else:\n",
    "            # get target from target DQN (Q')\n",
    "            Q[0, action] = reward + dis * np.max(targetDQN.predict(next_state))\n",
    "\n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        x_stack = np.vstack( [x_stack, state])\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(x_stack, y_stack)\n",
    "\n",
    "def ddqn_replay_train(mainDQN, targetDQN, train_batch):\n",
    "    '''\n",
    "    Double DQN implementation\n",
    "    :param mainDQN: main DQN\n",
    "    :param targetDQN: target DQN\n",
    "    :param train_batch: minibatch for train\n",
    "    :return: loss\n",
    "    '''\n",
    "    x_stack = np.empty(0).reshape(0, mainDQN.input_size)\n",
    "    y_stack = np.empty(0).reshape(0, mainDQN.output_size)\n",
    "\n",
    "    # Get stored information from the buffer\n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        if state is None:                                                               #####why does this happen?\n",
    "            print(\"None State, \", action, \" , \", reward, \" , \", next_state, \" , \", done)\n",
    "        else:\n",
    "            Q = mainDQN.predict(state)\n",
    "\n",
    "            # terminal?\n",
    "            if done:\n",
    "                Q[0, action] = reward\n",
    "            else:\n",
    "                # Double DQN: y = r + gamma * targetDQN(s')[a] where\n",
    "                # a = argmax(mainDQN(s'))\n",
    "                # Q[0, action] = reward + dis * targetDQN.predict(next_state)[0, np.argmax(mainDQN.predict(next_state))]\n",
    "                Q[0, action] = reward + dis * np.max(targetDQN.predict(next_state)) #####use normal one for now\n",
    "\n",
    "            y_stack = np.vstack([y_stack, Q])\n",
    "            x_stack = np.vstack([x_stack, state.reshape(-1, mainDQN.input_size)])   #####change shape to fit to super mario\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(x_stack, y_stack)\n",
    "\n",
    "def get_copy_var_ops(*, dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "\n",
    "    # Copy variables src_scope to dest_scope\n",
    "    op_holder = []\n",
    "\n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "def bot_play(mainDQN, env=env):\n",
    "    # See our trained network in action\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            break\n",
    "\n",
    "def main():\n",
    "    max_episodes = 5000\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
    "        targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        #initial copy q_net -> target_net\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\")\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(max_episodes):\n",
    "            e = 1. / ((episode / 10) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand(1) < e or state is None or state.size == 1:           #####why does this happen?\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Choose an action by greedily from the Q-network\n",
    "                    #action = np.argmax(mainDQN.predict(state))\n",
    "                    action = mainDQN.predict(state).flatten().tolist()                  #####flatten it and change it as a list\n",
    "                    for i in range(len(action)):                                        #####the action list has to have only integer 1 or 0\n",
    "                        if action[i] > 0.5 :\n",
    "                            action[i] = 1                                               #####integer 1 only, no 1.0\n",
    "                        else:\n",
    "                            action[i] = 0                                               #####integer 0 only, no 0.0\n",
    "\n",
    "                # Get new state and reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                if done: # Penalty\n",
    "                    reward = -100\n",
    "\n",
    "                # Save the experience to our buffer\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                      replay_buffer.popleft()\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "                if step_count > 10000:   # Good enough. Let's move on\n",
    "                    break\n",
    "\n",
    "            print(\"Episode: {} steps: {}\".format(episode, step_count))\n",
    "            if step_count > 10000:\n",
    "                pass\n",
    "                # break\n",
    "\n",
    "            if episode % 10 == 1: # train every 10 episode\n",
    "                # Get a random batch of experiences\n",
    "                for _ in range(50):\n",
    "                    minibatch = random.sample(replay_buffer, 10)\n",
    "                    loss, _ = ddqn_replay_train(mainDQN, targetDQN, minibatch)\n",
    "\n",
    "                print(\"Loss: \", loss)\n",
    "                # copy q_net -> target_net\n",
    "                sess.run(copy_ops)\n",
    "\n",
    "        # See our trained bot in action\n",
    "        env2 = wrappers.Monitor(env, 'gym-results', force=True)\n",
    "\n",
    "        for i in range(200):\n",
    "            bot_play(mainDQN, env=env2)\n",
    "\n",
    "        env2.close()\n",
    "        # gym.upload(\"gym-results\", api_key=\"sk_VT2wPcSSOylnlPORltmQ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
